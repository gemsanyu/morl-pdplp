Timer unit: 1e-06 s

Total time: 29.788 s
File: /home/m381067/morl-pdplp-ham-no-coords/bpdplp/bpdplp_env.py
Function: act at line 214

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   214                                               @profile
   215                                               def act(self, batch_idx, selected_vecs, selected_nodes):
   216                                                   #just send the vehicle to the node
   217                                                   # selected_nodes = np.asanyarray(selected_nodes)
   218                                                   # selected_vecs = np.asanyarray(selected_vecs)
   219      9600   26517557.9   2762.2     89.0          reward = self.service_node_by_vec(batch_idx, selected_vecs, selected_nodes)
   220      9600    3270424.9    340.7     11.0          return *self.get_state(), reward

Total time: 26.2275 s
File: /home/m381067/morl-pdplp-ham-no-coords/bpdplp/bpdplp_env.py
Function: service_node_by_vec at line 227

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   227                                               @profile
   228                                               def service_node_by_vec(self, batch_idx, selected_vecs, selected_nodes):
   229      9600       5191.0      0.5      0.0          travel_time_list = self.travel_time_list
   230      9600      86564.5      9.0      0.3          travel_time_vecs = travel_time_list[batch_idx, selected_vecs, selected_nodes]
   231      9600       3703.9      0.4      0.0          f1 = travel_time_vecs
   232      9600      53825.9      5.6      0.2          self.is_node_visited[batch_idx, selected_nodes] = True
   233                                                   # isnp -> is_selected_node_pickup
   234                                                   # assign the request to the vehicles
   235      9600      67432.4      7.0      0.3          isnp = selected_nodes <= self.num_requests
   236                                                   # not_served = self.request_assignment[batch_idx[isnp], selected_nodes[isnp]-1] == -1
   237                                                   # assert np.all(not_served)
   238      9600     121702.5     12.7      0.5          self.request_assignment[batch_idx[isnp], selected_nodes[isnp]-1] = selected_vecs[isnp]
   239                                                   #add demands
   240                                                   # isnd = selected_nodes > self.num_requests
   241                                                   # selected_requests = selected_nodes.copy()
   242                                                   # selected_requests[isnd] -= self.num_requests
   243                                                   # assert np.all(self.request_assignment[batch_idx, selected_requests-1] == selected_vecs)
   244      9600      40677.9      4.2      0.2          selected_nodes_demands = self.demands[batch_idx, selected_nodes]
   245      9600     107301.0     11.2      0.4          self.current_load[batch_idx, selected_vecs] += selected_nodes_demands
   246      9600      89360.8      9.3      0.3          self.num_visited_nodes[batch_idx, selected_vecs] += 1
   247      9600      69962.3      7.3      0.3          self.tour_list[batch_idx, selected_vecs, self.num_visited_nodes[batch_idx, selected_vecs]] = selected_nodes
   248      9600     144108.0     15.0      0.5          self.departure_time_list[batch_idx, selected_vecs, self.num_visited_nodes[batch_idx, selected_vecs]] = self.current_time[batch_idx, selected_vecs]
   249                                                   #add to travel time to current time
   250      9600      70385.5      7.3      0.3          self.current_time[batch_idx, selected_vecs] += travel_time_vecs
   251                                           
   252                                                   # now filter the actions or the selected vehicles based on their current time
   253                                                   # if early, then ceil,
   254                                                   # if late, then add to penalty 
   255      9600       4520.4      0.5      0.0          f2 = None 
   256      9600      28447.7      3.0      0.1          selected_vecs_current_time = self.current_time[batch_idx, selected_vecs]
   257      9600     129811.9     13.5      0.5          selected_nodes_tw = self.time_windows[batch_idx, selected_nodes]
   258      9600      42421.9      4.4      0.2          is_too_early = selected_vecs_current_time <= selected_nodes_tw[:,0]
   259      9600     189165.2     19.7      0.7          if np.any(is_too_early):
   260      1892      20635.9     10.9      0.1              self.current_time[batch_idx[is_too_early], selected_vecs[is_too_early]] = selected_nodes_tw[is_too_early,0]
   261      9600      39228.2      4.1      0.1          is_too_late = selected_vecs_current_time > selected_nodes_tw[:,1]
   262      9600      87146.7      9.1      0.3          late_penalty = (selected_vecs_current_time[is_too_late]-selected_nodes_tw[is_too_late,1])
   263      9600       8440.3      0.9      0.0          if len(late_penalty)>0:
   264      9479     115755.4     12.2      0.4              self.late_penalty[batch_idx[is_too_late], selected_vecs[is_too_late]] += late_penalty
   265                                                       # f2[is_too_late] = late_penalty
   266      9479      38723.2      4.1      0.1              f2 = np.empty_like(f1)
   267      9479      17674.9      1.9      0.1              f2[is_too_late] = late_penalty
   268      9479      35431.0      3.7      0.1              f2[np.logical_not(is_too_late)] = 0
   269      9600       5393.2      0.6      0.0          if f2 is None:
   270       121       2582.4     21.3      0.0              f2 = np.zeros_like(f1)
   271                                                   
   272      9600      92159.2      9.6      0.4          self.arrived_time[batch_idx, selected_vecs, self.num_visited_nodes[batch_idx, selected_vecs]] = self.current_time[batch_idx, selected_vecs]
   273      9600      74165.3      7.7      0.3          self.travel_cost[batch_idx, selected_vecs] += travel_time_vecs 
   274      9600      28735.5      3.0      0.1          self.current_location_idx[batch_idx, selected_vecs] = selected_nodes
   275                                                   # after arriving, and start service, add service time to current time
   276      9600     137704.7     14.3      0.5          self.current_time[batch_idx, selected_vecs] += self.service_durations[batch_idx, selected_nodes]
   277      9600   24155436.7   2516.2     92.1          self.travel_time_list = self.get_travel_time()
   278      9600     113737.8     11.8      0.4          return np.concatenate([f1[:, np.newaxis], f2[:, np.newaxis]], axis=-1)

Total time: 3.1714 s
File: /home/m381067/morl-pdplp-ham-no-coords/bpdplp/bpdplp_env.py
Function: get_state at line 280

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   280                                               @profile
   281                                               def get_state(self):
   282      9600     673354.3     70.1     21.2          vdf = self.vehicle_dynamic_features
   283      9600     707549.4     73.7     22.3          ndf = self.node_dynamic_features
   284      9600    1785590.2    186.0     56.3          fm = self.feasibility_mask
   285      9600       4901.1      0.5      0.2          return vdf, ndf, fm

Total time: 2.44007 s
File: /home/m381067/morl-pdplp-ham-no-coords/utils.py
Function: encode at line 40

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    40                                           @profile
    41                                           def encode(agent, static_features):
    42        96        428.2      4.5      0.0      num_requests = int((static_features.shape[1]-1)//2)
    43        96       3226.2     33.6      0.1      depot_static_features = static_features[:, 0].unsqueeze(1)
    44        96       1004.4     10.5      0.0      delivery_static_features = static_features[:,num_requests+1:]
    45        96        892.2      9.3      0.0      pickup_only_static_features = static_features[:,1:num_requests+1]
    46                                               
    47        96       1272.2     13.3      0.1      depot_spatial_other_features = depot_static_features[:,:,:4]
    48        96       1263.4     13.2      0.1      delivery_spatial_other_features = delivery_static_features[:,:,:4]
    49        96       6900.4     71.9      0.3      pickup_spatial_other_features = torch.cat([pickup_only_static_features[:,:,:4], delivery_static_features[:,:,:4]], dim=2) 
    50        96     316842.2   3300.4     13.0      depot_so_init_embedding = agent.depot_spatial_other_embedder(depot_spatial_other_features)
    51        96       9364.8     97.6      0.4      pickup_so_init_embedding = agent.pick_spatial_other_embedder(pickup_spatial_other_features)
    52        96       7684.1     80.0      0.3      delivery_so_init_embedding = agent.delivery_spatial_other_embedder(delivery_spatial_other_features)
    53        96       2685.2     28.0      0.1      node_so_init_embeddings = torch.cat([depot_so_init_embedding, pickup_so_init_embedding, delivery_so_init_embedding], dim=1)
    54        96    1054047.3  10979.7     43.2      node_so_embeddings, graph_so_embeddings = agent.spatial_other_gae(node_so_init_embeddings)
    55                                               
    56        96       1598.8     16.7      0.1      depot_time_windows = depot_static_features[:,:,4:]
    57        96       1177.4     12.3      0.0      delivery_time_windows = delivery_static_features[:,:,4:]
    58        96       3251.3     33.9      0.1      pickup_time_windows = torch.cat([pickup_only_static_features[:,:,4:], delivery_time_windows], dim=2)
    59        96       5776.9     60.2      0.2      depot_temporal_init_embedding = agent.depot_temporal_embedder(depot_time_windows)
    60        96       5715.0     59.5      0.2      pickup_temporal_init_embedding = agent.pick_temporal_embedder(pickup_time_windows)
    61        96       5968.8     62.2      0.2      delivery_temporal_init_embedding = agent.delivery_temporal_embedder(delivery_time_windows)
    62        96       1968.9     20.5      0.1      node_temporal_init_embeddings = torch.cat([depot_temporal_init_embedding, pickup_temporal_init_embedding, delivery_temporal_init_embedding], dim=1)
    63        96       1525.6     15.9      0.1      node_init_embeddings = torch.cat([node_so_embeddings, node_temporal_init_embeddings], dim=2)
    64        96     992018.8  10333.5     40.7      node_temporal_embeddings, graph_temporal_embeddings = agent.temporal_gae(node_init_embeddings)
    65                                               
    66        96       1222.7     12.7      0.1      node_embeddings = node_so_embeddings + node_temporal_embeddings
    67        96       1035.1     10.8      0.0      graph_embeddings = graph_so_embeddings + graph_temporal_embeddings
    68        96       3501.8     36.5      0.1      fixed_context = agent.project_fixed_context(graph_embeddings)
    69        96       5249.3     54.7      0.2      glimpse_K_static, glimpse_V_static, logits_K_static = agent.project_embeddings(node_embeddings).chunk(3, dim=-1)
    70        96       2541.6     26.5      0.1      glimpse_K_static = agent._make_heads(glimpse_K_static)
    71        96       1858.7     19.4      0.1      glimpse_V_static = agent._make_heads(glimpse_V_static)
    72        96         51.4      0.5      0.0      return node_embeddings, fixed_context, glimpse_K_static, glimpse_V_static, logits_K_static

Total time: 50.2458 s
File: /home/m381067/morl-pdplp-ham-no-coords/utils.py
Function: solve_decode_only at line 90

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    90                                           @profile
    91                                           def solve_decode_only(agent, env:BPDPLP_Env, node_embeddings, fixed_context, glimpse_K_static, glimpse_V_static, logits_K_static, param_dict=None):
    92        96        210.8      2.2      0.0      batch_size, num_nodes, embed_dim = node_embeddings.shape
    93        96        618.6      6.4      0.0      batch_idx = np.arange(batch_size)
    94        96       1925.9     20.1      0.0      sum_logprobs = torch.zeros((batch_size,), device=agent.device, dtype=torch.float32)
    95        96       1193.8     12.4      0.0      sum_entropies = torch.zeros((batch_size,), device=agent.device, dtype=torch.float32)
    96        96     355455.8   3702.7      0.7      static_features, vehicle_dynamic_features, node_dynamic_features, feasibility_mask = env.begin()
    97        96    2508257.1  26127.7      5.0      vehicle_dynamic_features = torch.from_numpy(vehicle_dynamic_features).to(agent.device, dtype=torch.float32)
    98        96       8425.4     87.8      0.0      node_dynamic_features = torch.from_numpy(node_dynamic_features).to(agent.device, dtype=torch.float32)
    99        96       2907.4     30.3      0.0      feasibility_mask = torch.from_numpy(feasibility_mask).to(agent.device, dtype=bool)
   100        96         73.5      0.8      0.0      num_vehicles = env.num_vehicles
   101        96       2686.6     28.0      0.0      max_num_vehicles = int(np.max(num_vehicles))
   102        96       2369.6     24.7      0.0      num_vehicles_cum = np.concatenate([np.asanyarray([0]),np.cumsum(num_vehicles)])
   103        96      30944.0    322.3      0.1      vehicle_batch_idx = np.concatenate([ np.asanyarray([i]*num_vehicles[i]) for i in range(batch_size)])
   104        96      42796.5    445.8      0.1      vehicle_idx = np.concatenate([np.arange(num_vehicles[i]) for i in range(batch_size)])
   105                                               # expanding glimpses
   106        96       2232.3     23.3      0.0      glimpse_V_static = glimpse_V_static.unsqueeze(2).expand(-1,-1,max_num_vehicles,-1,-1)
   107        96       1076.8     11.2      0.0      glimpse_K_static = glimpse_K_static.unsqueeze(2).expand(-1,-1,max_num_vehicles,-1,-1)
   108        96       1017.5     10.6      0.0      logits_K_static = logits_K_static.unsqueeze(1).expand(-1,max_num_vehicles,-1,-1)
   109        96        923.7      9.6      0.0      fixed_context = fixed_context.unsqueeze(1).expand(-1,max_num_vehicles,-1)
   110                                               
   111        96         40.2      0.4      0.0      reward_list = []
   112        96         32.4      0.3      0.0      logprob_list = []
   113      9696     403771.0     41.6      0.8      while torch.any(feasibility_mask):
   114      9600     977870.0    101.9      1.9          prev_node_embeddings = node_embeddings[env.batch_vec_idx, env.current_location_idx.flatten(), :]
   115      9600      91771.6      9.6      0.2          prev_node_embeddings = prev_node_embeddings.view((batch_size,max_num_vehicles,-1))
   116     19200   11559943.0    602.1     23.0          forward_results = agent.forward(node_embeddings,
   117      9600       3569.6      0.4      0.0                                          fixed_context,
   118      9600       3482.5      0.4      0.0                                          prev_node_embeddings,
   119      9600       3793.5      0.4      0.0                                          node_dynamic_features,
   120      9600       3730.6      0.4      0.0                                          vehicle_dynamic_features,
   121      9600       3572.5      0.4      0.0                                          glimpse_V_static,
   122      9600       3488.6      0.4      0.0                                          glimpse_K_static,
   123      9600       3397.7      0.4      0.0                                          logits_K_static,
   124      9600       3400.2      0.4      0.0                                          feasibility_mask,
   125      9600       3355.0      0.3      0.0                                          param_dict)
   126      9600      67361.2      7.0      0.1          selected_vecs, selected_nodes, logprobs, entropy_list = forward_results
   127      9600    1917638.5    199.8      3.8          selected_vecs = selected_vecs.cpu().numpy()
   128      9600     203170.8     21.2      0.4          selected_nodes = selected_nodes.cpu().numpy()
   129      9600   29961129.5   3121.0     59.6          vehicle_dynamic_features, node_dynamic_features, feasibility_mask, reward = env.act(batch_idx, selected_vecs, selected_nodes)
   130      9600     223679.5     23.3      0.4          logprob_list += [logprobs[:, np.newaxis]]
   131      9600      16163.1      1.7      0.0          reward_list += [reward[:, np.newaxis, :]]
   132      9600     519509.8     54.1      1.0          vehicle_dynamic_features = torch.from_numpy(vehicle_dynamic_features).to(agent.device, dtype=torch.float32)
   133      9600     700320.7     73.0      1.4          node_dynamic_features = torch.from_numpy(node_dynamic_features).to(agent.device, dtype=torch.float32)
   134      9600     290154.9     30.2      0.6          feasibility_mask = torch.from_numpy(feasibility_mask).to(agent.device, dtype=bool)
   135                                                   
   136        96      42578.3    443.5      0.1      reward_list = np.concatenate(reward_list, axis=1)
   137        96      34712.0    361.6      0.1      logprob_list = torch.concatenate(logprob_list, dim=1)
   138        96     240897.7   2509.4      0.5      tour_list, arrived_time_list, departure_time_list, travel_costs, late_penalties = env.finish()
   139        96        134.0      1.4      0.0      return tour_list, arrived_time_list, departure_time_list, travel_costs, late_penalties, reward_list, logprob_list, sum_entropies

Total time: 63.2942 s
File: train_single.py
Function: train_one_epoch at line 19

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    19                                           @profile
    20                                           def train_one_epoch(args, agent, best_agent, opt, train_dataset, tb_writer, epoch):
    21         4       4308.9   1077.2      0.0      agent.train()
    22         4       3697.3    924.3      0.0      best_agent.eval()
    23         4          3.1      0.8      0.0      actual_batch_size = 128
    24         4          2.2      0.5      0.0      propagated_batch_size = 0
    25         4        863.2    215.8      0.0      train_dataloader = DataLoader(train_dataset, batch_size=actual_batch_size, shuffle=True, num_workers=2, pin_memory=True)
    26         4          1.3      0.3      0.0      sum_advantage = 0
    27         4          1.1      0.3      0.0      sum_training_travel_costs = 0
    28         4          1.1      0.3      0.0      sum_training_penalties = 0
    29         4          1.0      0.3      0.0      sum_training_entropies = 0
    30                                               
    31        36    4712449.2 130901.4      7.4      for batch_idx, batch in tqdm(enumerate(train_dataloader), desc=f'Training epoch {epoch}'):
    32        32        314.9      9.8      0.0          _, num_vehicles, max_capacity, coords, norm_coords, demands, norm_demands, planning_time, time_windows, norm_time_windows, service_durations, norm_service_durations, distance_matrix, norm_distance_matrix, road_types = batch
    33        32     224978.1   7030.6      0.4          env = BPDPLP_Env(num_vehicles, max_capacity, coords, norm_coords, demands, norm_demands, planning_time, time_windows, norm_time_windows, service_durations, norm_service_durations, distance_matrix, norm_distance_matrix, road_types)
    34        32     140397.4   4387.4      0.2          static_features,_,_,_ = env.begin()
    35        32       4356.6    136.1      0.0          static_features = torch.from_numpy(static_features).to(agent.device)
    36        32    1107957.6  34623.7      1.8          encode_results = encode(agent, static_features)
    37        32        323.0     10.1      0.0          node_embeddings, fixed_context, glimpse_K_static, glimpse_V_static, logits_K_static = encode_results
    38        32   19855157.9 620473.7     31.4          solve_results = solve_decode_only(agent, env, node_embeddings, fixed_context, glimpse_K_static, glimpse_V_static, logits_K_static)
    39                                                   
    40        32        279.3      8.7      0.0          tour_list, arrived_time_list, departure_time_list, travel_costs, late_penalties, reward_list, logprob_list, sum_entropies = solve_results
    41                                                   
    42                                                   # computing critic
    43        32        875.5     27.4      0.0          with torch.no_grad():
    44        32     684670.2  21395.9      1.1              encode_results = encode(best_agent, static_features)
    45        32        303.0      9.5      0.0              node_embeddings, fixed_context, glimpse_K_static, glimpse_V_static, logits_K_static = encode_results
    46        32   18256233.1 570507.3     28.8              greedy_solve_results = solve_decode_only(best_agent, env, node_embeddings, fixed_context, glimpse_K_static, glimpse_V_static, logits_K_static)    
    47        32        476.0     14.9      0.0              _, _, _, greedy_travel_costs, greedy_late_penalties, _, _, _ = greedy_solve_results
    48                                           
    49        32         99.2      3.1      0.0          score = travel_costs + late_penalties
    50        32         53.1      1.7      0.0          greedy_score = greedy_travel_costs + greedy_late_penalties
    51        32         80.3      2.5      0.0          tc_adv = travel_costs-greedy_travel_costs
    52        32         55.5      1.7      0.0          lp_adv = late_penalties-greedy_late_penalties
    53        32       4004.4    125.1      0.0          tc_adv = (tc_adv-tc_adv.mean())/tc_adv.std()
    54        32       2361.2     73.8      0.0          lp_adv = (lp_adv-lp_adv.mean())/lp_adv.std()
    55        32         58.1      1.8      0.0          advantage_list = tc_adv + lp_adv       
    56        32       1345.4     42.0      0.0          logprob_list = logprob_list.sum(dim=-1)
    57        32     167885.5   5246.4      0.3          loss = logprob_list*torch.from_numpy(advantage_list).to(agent.device)
    58        32       3110.6     97.2      0.0          loss = loss.mean() - 0.05*sum_entropies.mean()
    59        32   12148620.1 379644.4     19.2          loss.backward()
    60        32         74.5      2.3      0.0          propagated_batch_size += actual_batch_size
    61        32         97.5      3.0      0.0          if propagated_batch_size == args.batch_size:
    62        32         44.9      1.4      0.0              propagated_batch_size = 0
    63        32    5961377.3 186293.0      9.4              update_step_only(agent, opt, args.max_grad_norm)
    64                                                   
    65        32       1050.1     32.8      0.0          sum_advantage += advantage_list.sum()
    66        32        162.0      5.1      0.0          sum_training_travel_costs += travel_costs.sum()
    67        32        139.0      4.3      0.0          sum_training_penalties += late_penalties.sum()
    68        32       2648.9     82.8      0.0          sum_training_entropies += sum_entropies.sum().detach().cpu()
    69                                               
    70         4        968.1    242.0      0.0      tb_writer.add_scalar("Training Advantage", sum_advantage/args.num_training_samples, epoch)
    71         4        247.0     61.7      0.0      tb_writer.add_scalar("Training Travel Costs", sum_training_travel_costs/args.num_training_samples, epoch)
    72         4        214.7     53.7      0.0      tb_writer.add_scalar("Training Late Penalties", sum_training_penalties/args.num_training_samples, epoch)
    73         4        210.3     52.6      0.0      tb_writer.add_scalar("Training Score", (sum_training_penalties+sum_training_travel_costs)/args.num_training_samples, epoch)
    74         4       1654.1    413.5      0.0      tb_writer.add_scalar("Training Entropies", sum_training_entropies/args.num_training_samples, epoch)

