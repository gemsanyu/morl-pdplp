Timer unit: 1e-06 s

Total time: 28.5245 s
File: /home/m381067/morl-pdplp-ham-no-coords/bpdplp/bpdplp_env.py
Function: act at line 216

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   216                                               @profile
   217                                               def act(self, batch_idx, selected_vecs, selected_nodes):
   218                                                   #just send the vehicle to the node
   219                                                   # selected_nodes = np.asanyarray(selected_nodes)
   220                                                   # selected_vecs = np.asanyarray(selected_vecs)
   221      9600   25263297.2   2631.6     88.6          reward = self.service_node_by_vec(batch_idx, selected_vecs, selected_nodes)
   222      9600    3261182.3    339.7     11.4          return *self.get_state(), reward

Total time: 25.001 s
File: /home/m381067/morl-pdplp-ham-no-coords/bpdplp/bpdplp_env.py
Function: service_node_by_vec at line 229

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   229                                               @profile
   230                                               def service_node_by_vec(self, batch_idx, selected_vecs, selected_nodes):
   231      9600       5265.8      0.5      0.0          travel_time_list = self.travel_time_list
   232      9600      82778.2      8.6      0.3          travel_time_vecs = travel_time_list[batch_idx, selected_vecs, selected_nodes]
   233      9600       3285.0      0.3      0.0          f1 = travel_time_vecs
   234      9600      50446.4      5.3      0.2          self.is_node_visited[batch_idx, selected_nodes] = True
   235                                                   # isnp -> is_selected_node_pickup
   236                                                   # assign the request to the vehicles
   237      9600      66493.1      6.9      0.3          isnp = selected_nodes <= self.num_requests
   238                                                   # not_served = self.request_assignment[batch_idx[isnp], selected_nodes[isnp]-1] == -1
   239                                                   # assert np.all(not_served)
   240      9600     120183.1     12.5      0.5          self.request_assignment[batch_idx[isnp], selected_nodes[isnp]-1] = selected_vecs[isnp]
   241                                                   #add demands
   242                                                   # isnd = selected_nodes > self.num_requests
   243                                                   # selected_requests = selected_nodes.copy()
   244                                                   # selected_requests[isnd] -= self.num_requests
   245                                                   # assert np.all(self.request_assignment[batch_idx, selected_requests-1] == selected_vecs)
   246      9600      39358.6      4.1      0.2          selected_nodes_demands = self.demands[batch_idx, selected_nodes]
   247      9600     102620.0     10.7      0.4          self.current_load[batch_idx, selected_vecs] += selected_nodes_demands
   248      9600      84746.2      8.8      0.3          self.num_visited_nodes[batch_idx, selected_vecs] += 1
   249      9600      59812.2      6.2      0.2          self.tour_list[batch_idx, selected_vecs, self.num_visited_nodes[batch_idx, selected_vecs]] = selected_nodes
   250      9600     153136.9     16.0      0.6          self.departure_time_list[batch_idx, selected_vecs, self.num_visited_nodes[batch_idx, selected_vecs]] = self.current_time[batch_idx, selected_vecs]
   251                                                   #add to travel time to current time
   252      9600      74641.2      7.8      0.3          self.current_time[batch_idx, selected_vecs] += travel_time_vecs
   253                                           
   254                                                   # now filter the actions or the selected vehicles based on their current time
   255                                                   # if early, then ceil,
   256                                                   # if late, then add to penalty 
   257      9600       4143.0      0.4      0.0          f2 = None 
   258      9600      26195.4      2.7      0.1          selected_vecs_current_time = self.current_time[batch_idx, selected_vecs]
   259      9600     126867.6     13.2      0.5          selected_nodes_tw = self.time_windows[batch_idx, selected_nodes]
   260      9600      40545.9      4.2      0.2          is_too_early = selected_vecs_current_time <= selected_nodes_tw[:,0]
   261      9600     192193.5     20.0      0.8          if np.any(is_too_early):
   262      2489      25372.7     10.2      0.1              self.current_time[batch_idx[is_too_early], selected_vecs[is_too_early]] = selected_nodes_tw[is_too_early,0]
   263      9600      37288.3      3.9      0.1          is_too_late = selected_vecs_current_time > selected_nodes_tw[:,1]
   264      9600      84057.9      8.8      0.3          late_penalty = (selected_vecs_current_time[is_too_late]-selected_nodes_tw[is_too_late,1])
   265      9600       8828.5      0.9      0.0          if len(late_penalty)>0:
   266      9467     109209.3     11.5      0.4              self.late_penalty[batch_idx[is_too_late], selected_vecs[is_too_late]] += late_penalty
   267                                                       # f2[is_too_late] = late_penalty
   268      9467      38370.9      4.1      0.2              f2 = np.empty_like(f1)
   269      9467      16957.5      1.8      0.1              f2[is_too_late] = late_penalty
   270      9467      35592.5      3.8      0.1              f2[np.logical_not(is_too_late)] = 0
   271      9600       5152.4      0.5      0.0          if f2 is None:
   272       133       2796.7     21.0      0.0              f2 = np.zeros_like(f1)
   273                                                   
   274      9600      87228.4      9.1      0.3          self.arrived_time[batch_idx, selected_vecs, self.num_visited_nodes[batch_idx, selected_vecs]] = self.current_time[batch_idx, selected_vecs]
   275      9600      70847.0      7.4      0.3          self.travel_cost[batch_idx, selected_vecs] += travel_time_vecs 
   276      9600      26212.4      2.7      0.1          self.current_location_idx[batch_idx, selected_vecs] = selected_nodes
   277                                                   # after arriving, and start service, add service time to current time
   278      9600     130286.3     13.6      0.5          self.current_time[batch_idx, selected_vecs] += self.service_durations[batch_idx, selected_nodes]
   279      9600   22978894.8   2393.6     91.9          self.travel_time_list = self.get_travel_time()
   280      9600     111216.9     11.6      0.4          return np.concatenate([f1[:, np.newaxis], f2[:, np.newaxis]], axis=-1)

Total time: 3.17 s
File: /home/m381067/morl-pdplp-ham-no-coords/bpdplp/bpdplp_env.py
Function: get_state at line 282

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   282                                               @profile
   283                                               def get_state(self):
   284      9600     775838.5     80.8     24.5          vehicle_dynamic_features = self.vehicle_dynamic_features
   285      9600     667917.5     69.6     21.1          node_dynamic_features = self.node_dynamic_features
   286      9600    1721560.2    179.3     54.3          feasibility_mask = self.feasibility_mask 
   287      9600       4683.7      0.5      0.1          return vehicle_dynamic_features, node_dynamic_features, feasibility_mask

Total time: 1.59928 s
File: /home/m381067/morl-pdplp-ham-no-coords/utils.py
Function: encode at line 37

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    37                                           @profile
    38                                           def encode(agent, static_features):
    39        96        411.4      4.3      0.0      num_requests = int((static_features.shape[1]-1)//2)
    40        96       3165.4     33.0      0.2      depot_static_features = static_features[:, 0].unsqueeze(1)
    41        96        898.5      9.4      0.1      delivery_static_features = static_features[:,num_requests+1:]
    42        96       5079.1     52.9      0.3      pickup_static_features = torch.concat([static_features[:,1:num_requests+1], delivery_static_features], dim=2)
    43        96     318125.8   3313.8     19.9      depot_init_embedding = agent.depot_embedder(depot_static_features)
    44        96       9170.8     95.5      0.6      pickup_init_embedding = agent.pick_embedder(pickup_static_features)
    45        96       7262.2     75.6      0.5      delivery_init_embedding = agent.delivery_embedder(delivery_static_features)
    46        96       2638.1     27.5      0.2      node_init_embeddings = torch.concat([depot_init_embedding, pickup_init_embedding, delivery_init_embedding], dim=1)
    47        96    1238787.7  12904.0     77.5      node_embeddings, graph_embeddings = agent.gae(node_init_embeddings)
    48        96       3454.9     36.0      0.2      fixed_context = agent.project_fixed_context(graph_embeddings)
    49        96       6117.6     63.7      0.4      glimpse_K_static, glimpse_V_static, logits_K_static = agent.project_embeddings(node_embeddings).chunk(3, dim=-1)
    50        96       2401.3     25.0      0.2      glimpse_K_static = agent._make_heads(glimpse_K_static)
    51        96       1723.7     18.0      0.1      glimpse_V_static = agent._make_heads(glimpse_V_static)
    52        96         46.9      0.5      0.0      return node_embeddings, fixed_context, glimpse_K_static, glimpse_V_static, logits_K_static

Total time: 47.4824 s
File: /home/m381067/morl-pdplp-ham-no-coords/utils.py
Function: solve_decode_only at line 70

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    70                                           @profile
    71                                           def solve_decode_only(agent, env:BPDPLP_Env, node_embeddings, fixed_context, glimpse_K_static, glimpse_V_static, logits_K_static, param_dict=None):
    72        96        191.7      2.0      0.0      batch_size, num_nodes, embed_dim = node_embeddings.shape
    73        96        611.1      6.4      0.0      batch_idx = np.arange(batch_size)
    74                                               # sum_logprobs = torch.zeros((batch_size,), device=agent.device, dtype=torch.float32)
    75        96       1888.0     19.7      0.0      sum_entropies = torch.zeros((batch_size,), device=agent.device, dtype=torch.float32)
    76        96     344004.2   3583.4      0.7      static_features, vehicle_dynamic_features, node_dynamic_features, feasibility_mask = env.begin()
    77        96    1417002.9  14760.4      3.0      vehicle_dynamic_features = torch.from_numpy(vehicle_dynamic_features).to(agent.device, dtype=torch.float32)
    78        96       8525.5     88.8      0.0      node_dynamic_features = torch.from_numpy(node_dynamic_features).to(agent.device, dtype=torch.float32)
    79        96       2944.2     30.7      0.0      feasibility_mask = torch.from_numpy(feasibility_mask).to(agent.device, dtype=bool)
    80        96         72.8      0.8      0.0      num_vehicles = env.num_vehicles
    81        96       2703.6     28.2      0.0      max_num_vehicles = int(np.max(num_vehicles))
    82                                               # num_vehicles_cum = np.concatenate([np.asanyarray([0]),np.cumsum(num_vehicles)])
    83                                               # vehicle_batch_idx = np.concatenate([ np.asanyarray([i]*num_vehicles[i]) for i in range(batch_size)])
    84                                               # vehicle_idx = np.concatenate([np.arange(num_vehicles[i]) for i in range(batch_size)])
    85                                               # expanding glimpses
    86        96       2133.7     22.2      0.0      glimpse_V_static = glimpse_V_static.unsqueeze(2).expand(-1,-1,max_num_vehicles,-1,-1)
    87        96        979.9     10.2      0.0      glimpse_K_static = glimpse_K_static.unsqueeze(2).expand(-1,-1,max_num_vehicles,-1,-1)
    88        96       1014.1     10.6      0.0      logits_K_static = logits_K_static.unsqueeze(1).expand(-1,max_num_vehicles,-1,-1)
    89        96        887.0      9.2      0.0      fixed_context = fixed_context.unsqueeze(1).expand(-1,max_num_vehicles,-1)
    90        96         43.7      0.5      0.0      reward_list = []
    91        96         42.2      0.4      0.0      logprob_list = []
    92                                               
    93      9696     375003.7     38.7      0.8      while torch.any(feasibility_mask):
    94      9600     943550.2     98.3      2.0          prev_node_embeddings = node_embeddings[env.batch_vec_idx, env.current_location_idx.flatten(), :]
    95      9600      88300.7      9.2      0.2          prev_node_embeddings = prev_node_embeddings.view((batch_size,max_num_vehicles,-1))
    96     19200   11028611.9    574.4     23.2          forward_results = agent.forward(node_embeddings,
    97      9600       3430.9      0.4      0.0                                          fixed_context,
    98      9600       3178.4      0.3      0.0                                          prev_node_embeddings,
    99      9600       3226.2      0.3      0.0                                          node_dynamic_features,
   100      9600       3477.6      0.4      0.0                                          vehicle_dynamic_features,
   101      9600       3520.9      0.4      0.0                                          glimpse_V_static,
   102      9600       3264.5      0.3      0.0                                          glimpse_K_static,
   103      9600       3345.6      0.3      0.0                                          logits_K_static,
   104      9600       4461.9      0.5      0.0                                          feasibility_mask,
   105      9600       3327.8      0.3      0.0                                          param_dict=param_dict)
   106      9600      48083.2      5.0      0.1          selected_vecs, selected_nodes, logprobs, entropy_list = forward_results
   107      9600    2116082.1    220.4      4.5          selected_vecs = selected_vecs.cpu().numpy()
   108      9600     190115.0     19.8      0.4          selected_nodes = selected_nodes.cpu().numpy()
   109                                                   # env.act(batch_idx, selected_vecs, selected_nodes)
   110      9600   28687486.9   2988.3     60.4          vehicle_dynamic_features, node_dynamic_features, feasibility_mask, reward = env.act(batch_idx, selected_vecs, selected_nodes)
   111      9600     216715.6     22.6      0.5          logprob_list += [logprobs[:, np.newaxis]]
   112      9600      15897.8      1.7      0.0          reward_list += [reward[:, np.newaxis, :]]
   113      9600     234611.3     24.4      0.5          sum_entropies += entropy_list
   114                                                   # vehicle_dynamic_features, node_dynamic_features, feasibility_mask = env.get_state()
   115      9600     471533.4     49.1      1.0          vehicle_dynamic_features = torch.from_numpy(vehicle_dynamic_features).to(agent.device, dtype=torch.float32)
   116      9600     671045.2     69.9      1.4          node_dynamic_features = torch.from_numpy(node_dynamic_features).to(agent.device, dtype=torch.float32)
   117      9600     279200.7     29.1      0.6          feasibility_mask = torch.from_numpy(feasibility_mask).to(agent.device, dtype=bool)
   118                                                   
   119        96      39005.2    406.3      0.1      reward_list = np.concatenate(reward_list, axis=1)
   120        96      32656.6    340.2      0.1      logprob_list = torch.concatenate(logprob_list, dim=1)
   121        96     230047.4   2396.3      0.5      tour_list, arrived_time_list, departure_time_list, travel_costs, late_penalties = env.finish()
   122        96        133.4      1.4      0.0      return tour_list, arrived_time_list, departure_time_list, travel_costs, late_penalties, reward_list, logprob_list, sum_entropies

Total time: 56.8118 s
File: train_single.py
Function: train_one_epoch at line 19

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    19                                           @profile
    20                                           def train_one_epoch(args, agent, best_agent, opt, train_dataset, tb_writer, epoch):
    21         4       2450.9    612.7      0.0      agent.train()
    22         4       1942.3    485.6      0.0      best_agent.eval()
    23         4        744.6    186.2      0.0      train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2, pin_memory=True)
    24         4          1.3      0.3      0.0      sum_advantage = 0
    25         4          2.4      0.6      0.0      sum_training_travel_costs = 0
    26         4          1.1      0.3      0.0      sum_training_penalties = 0
    27         4          1.1      0.3      0.0      sum_training_entropies = 0
    28                                               
    29        36    4781473.7 132818.7      8.4      for batch_idx, batch in tqdm(enumerate(train_dataloader), desc=f'Training epoch {epoch}'):
    30        32        293.3      9.2      0.0          _, num_vehicles, max_capacity, coords, norm_coords, demands, norm_demands, planning_time, time_windows, norm_time_windows, service_durations, norm_service_durations, distance_matrix, norm_distance_matrix, road_types = batch
    31        32     224754.1   7023.6      0.4          env = BPDPLP_Env(num_vehicles, max_capacity, coords, norm_coords, demands, norm_demands, planning_time, time_windows, norm_time_windows, service_durations, norm_service_durations, distance_matrix, norm_distance_matrix, road_types)
    32        32     135881.7   4246.3      0.2          static_features,_,_,_ = env.begin()
    33        32       4374.9    136.7      0.0          static_features = torch.from_numpy(static_features).to(agent.device)
    34        32     950682.1  29708.8      1.7          encode_results = encode(agent, static_features)
    35        32        322.3     10.1      0.0          node_embeddings, fixed_context, glimpse_K_static, glimpse_V_static, logits_K_static = encode_results
    36        32   18974662.7 592958.2     33.4          solve_results = solve_decode_only(agent, env, node_embeddings, fixed_context, glimpse_K_static, glimpse_V_static, logits_K_static)
    37                                                   
    38        32        269.9      8.4      0.0          tour_list, arrived_time_list, departure_time_list, travel_costs, late_penalties, reward_list, logprob_list, sum_entropies = solve_results
    39                                                   
    40                                                   # computing critic
    41        32        828.4     25.9      0.0          with torch.no_grad():
    42        32     336699.3  10521.9      0.6              encode_results = encode(best_agent, static_features)
    43        32        298.9      9.3      0.0              node_embeddings, fixed_context, glimpse_K_static, glimpse_V_static, logits_K_static = encode_results
    44        32   17421247.4 544414.0     30.7              greedy_solve_results = solve_decode_only(best_agent, env, node_embeddings, fixed_context, glimpse_K_static, glimpse_V_static, logits_K_static)    
    45        32        432.7     13.5      0.0              _, _, _, greedy_travel_costs, greedy_late_penalties, _, _, _ = greedy_solve_results
    46                                           
    47        32         97.3      3.0      0.0          score = travel_costs + late_penalties
    48        32         47.3      1.5      0.0          greedy_score = greedy_travel_costs + greedy_late_penalties
    49        32         75.7      2.4      0.0          tc_adv = travel_costs-greedy_travel_costs
    50        32         52.7      1.6      0.0          lp_adv = late_penalties-greedy_late_penalties
    51        32       3947.1    123.3      0.0          tc_adv = (tc_adv-tc_adv.mean())/tc_adv.std()
    52        32       2252.2     70.4      0.0          lp_adv = (lp_adv-lp_adv.mean())/lp_adv.std()
    53        32         55.5      1.7      0.0          advantage_list = tc_adv + lp_adv
    54                                                   # advantage_list = score-greedy_score
    55                                                   # advantage_list = (advantage_list - advantage_list.mean())/advantage_list.std()
    56        32       1331.7     41.6      0.0          logprob_list = logprob_list.sum(dim=-1)
    57        32     152671.9   4771.0      0.3          loss = logprob_list*torch.from_numpy(advantage_list).to(agent.device)
    58        32       3044.4     95.1      0.0          loss = loss.mean() - 0.05*sum_entropies.mean()
    59        32   13803051.2 431345.4     24.3          update(agent, opt, loss, args.max_grad_norm)
    60                                                   
    61        32       1072.8     33.5      0.0          sum_advantage += advantage_list.sum()
    62        32        154.1      4.8      0.0          sum_training_travel_costs += travel_costs.sum()
    63        32        131.4      4.1      0.0          sum_training_penalties += late_penalties.sum()
    64        32       2691.9     84.1      0.0          sum_training_entropies += sum_entropies.sum().detach().cpu()
    65                                               
    66         4       1054.3    263.6      0.0      tb_writer.add_scalar("Training Advantage", sum_advantage/args.num_training_samples, epoch)
    67         4        287.2     71.8      0.0      tb_writer.add_scalar("Training Travel Costs", sum_training_travel_costs/args.num_training_samples, epoch)
    68         4        248.4     62.1      0.0      tb_writer.add_scalar("Training Late Penalties", sum_training_penalties/args.num_training_samples, epoch)
    69         4        241.6     60.4      0.0      tb_writer.add_scalar("Training Score", (sum_training_penalties+sum_training_travel_costs)/args.num_training_samples, epoch)
    70         4       1929.5    482.4      0.0      tb_writer.add_scalar("Training Entropies", sum_training_entropies/args.num_training_samples, epoch)

